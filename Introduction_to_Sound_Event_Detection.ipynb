{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 19596,
          "databundleVersionId": 1292430,
          "sourceType": "competition"
        },
        {
          "sourceId": 1262046,
          "sourceType": "datasetVersion",
          "datasetId": 726424
        },
        {
          "sourceId": 1305829,
          "sourceType": "datasetVersion",
          "datasetId": 755946
        },
        {
          "sourceId": 1306285,
          "sourceType": "datasetVersion",
          "datasetId": 756248
        },
        {
          "sourceId": 1306896,
          "sourceType": "datasetVersion",
          "datasetId": 756642
        },
        {
          "sourceId": 1307127,
          "sourceType": "datasetVersion",
          "datasetId": 756745
        },
        {
          "sourceId": 1307546,
          "sourceType": "datasetVersion",
          "datasetId": 756953
        },
        {
          "sourceId": 1346120,
          "sourceType": "datasetVersion",
          "datasetId": 782938
        },
        {
          "sourceId": 1405119,
          "sourceType": "datasetVersion",
          "datasetId": 821504
        }
      ],
      "dockerImageVersionId": 29987,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Introduction to Sound Event Detection",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nirmal369/jioq/blob/main/Introduction_to_Sound_Event_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'birdsong-recognition:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F19596%2F1292430%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T135615Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4bae03b9f85d25c496041155407c0455a9afba4c200205f0f1a9628e15a874e796bf99b23aa34d160a5127c10cb48d26bb2f5a42bb748b3124cae0c2631ae039c8129ec4afaa034dd34c90088b47524c829db97cdc9e607a2342b92dab68e476a586ab54f6036c8eb201e89979e39215f008b6504d445a9b2ad66190c3b669c5836b693ca65e4f2101deb19e15303fa263d3a43a80337c11214604cad180823b041b1f72131d1c307e0e3e458fc528a587e6b3d9992995a958ca6a4581e9a19e048d6270036c4a2b1cb05357591668ed3eddeff88db0c3c562d33fa7785343b79c4f3a6812174cb0c4543947df50f4f1eb9a37756841588250b5a3a25c27ba40,birdcall-check:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F726424%2F1262046%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T135615Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da501a7d45173b83be95417c20e8130ea679620c71891fe7b9611a866acfdad9c43746240a0c0d9011103e15e1d0d3961dfc7479ac291bc432267ce85c6dda10030f9084251996475ad21a7a53339bdc64f97f92509d1457f7e3d65a7f2ae1a7c6e1c2a2ee3f551e4177c959b72929805a5932360a502bfbb354406cca320df816a4a7e537b732113752a00bde2bf792d8da19dc61b72864a4fbcf53cea367ccf0ea7f5986154f7c1323f0b31bec793e016aea5fa87a89efc4a4d0f01b2d64fffaa47b2cc6939df33dc889b0adeb6e87446c63150fd4a1721c4659a8cd6db9b178896c7fd5248ef5445611a2aac68d58e34e0dd61df7c8c23d1b00177278605b8,birdsong-resampled-train-audio-00:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F755946%2F1305829%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T135615Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1976e42daf2b63732fa9527a0e8ba5798f44bf65ae352d44e1c081cb46df6e56e5d0fc842e4ee950245e565e8355499b62f77b40a8dcf09daf30738ce94df3d3dac1e641efe9d80d157236120868d75744db365069b9a17ffb886e12cf819333bc6be61ec18561a2fb3d4e1f1b8f149125fdaacbc9b7306af88cd29e147a81479e34b6c41fd7da595954ec66ad1c5b40f5c39461e7b6832db94a34778ed3ae73273f5373edb1122788f4becc41ed7d088193eb75342a9ada6ca172e8163938d89ab186ee5bb00c8ab2fcebe0d723a3e773ce11e773631d3b97280dda6a736a84056b8ce5459d0f4aadb8a3400e4ae12d593478da4fe6448e1ea45a1096ff6ff2,birdsong-resampled-train-audio-01:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F756248%2F1306285%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T135615Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db77e30ebab5fa86b3d26a5fc0227e188e1d4dd510e3313c73186060e6149abd9c4b6670e761643c5c1c5f8b49a5976be263fd440265084ee7d642f09a4e8600b59a7428c431e4f69a13125141c2ae8c07505e2445bf2d790f2fb206d52e5b585f8c26f14424d5c5828fa4cc8e3c52b8ccc98a01451ea889e2e40a98edab18bdc2145df66b7b33fcd07884190eb548b6700876bbc2c752f5e7b9a48c982ad33ee07b294b3862a2b7bbe980aafbecef0f1964fbed59fc94d121d6c3eae3b1e655d46c67bebded7fff8f8ebee690602c185264a3f68d476023f024ea189d5acb8ce3cd1a6b149d66894443f1a77317e5f29fa4f00784a51ea6b30c0ebdc0b8591b5,birdsong-resampled-train-audio-03:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F756642%2F1306896%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T135615Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7ddb2c2a16995da9b5581969c5aca676362a552bdb2a0883842c2b3d1fab5275b38208bc8fe8e5ace9f9b9667175e29f0eb40fa56d137b2f180b6ec6ee21872868608c5e6c11def75c797eaf7c0a5949a11d7e3c1863fa0d8703ff2fd0ed5a342ae44d2b3e909dd50bbb34b43a8baa12b1ac950635e051480f3ef792f0475df4ea77b83eee3555af8d82c99d172372bb1bd6cdfd1bf27446a1a34f9801166c7adefc32add577ac4ef80843d75a5eb5becc3f154c6ce56fa83e6b8d36bbf269ece77c35d958a349336c2b12ac47f46cb880db5a54ee48cc6bf3cb47f5b3973ad3c7c5a6492e6ca690b21c94642c13a0b423ca73508d86d34477b8444124b132b8,birdsong-resampled-train-audio-04:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F756745%2F1307127%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T135615Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0ddb95516f1e13c5baa7600fc3702a5c39710325bb09567f9a09faf82edf456d0519c88bbe54b74547ebac13278d45a8a1f817435ef2e171bcbb1e6106d2b582aa16dc8798647886aad3fb188d45d12b585917e262c96879a8b6c0beb140a8b5e8000e8ab838e6bbba67eaf5af3249d5024ed46c4e63f0997246c3ca9d1dfd1a0a0648c7b7254960bcabb2dd1e232b26bd50fbb0da7cd0dceaf3f60aacb04940095d1c8fdc1286565ba6e0adc548b1cabe6bf76e4dea57e22c18b38075cf0a4f20d63bebbbabd343dfb973fac568cec12f0bd98581b3a4f5f0e971e0b029476cbfdd62e36f1b85d9d9ecf08ed5894aad8d28296e30eef18f177e900ed144ee5c,birdsong-resampled-train-audio-02:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F756953%2F1307546%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T135615Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D60561dc625780fc2663c07c471dd4bb9f585b77b824fd68829c5533b0df28e00788a9f0a37832173b3e8e8e02c06ed13b3f36c3a1b7f5c3f51db6f911ba16c7dcc4fe43925ea7f35c176585fca2233dd2c9cf4c321b3456de78f2e7f69925380e4504a4097fd28969b891265fe64e6961bbd9939f7c85edaf91962b6ac9b520d98710cc37e9aafaa60025f2edc270853ffe617ede494863b81a58178831f3cd51bd424004220eb738496c0ad70575169452852730e6e21009360313ffb9a80c94a6917a835ce12b1e3060f2665fdc452828893b97bf6ddf21d03063832f8e45d95a0645034a1b9428d2d27b9477554a502e3dfc5f3b5fea899f83cdf05efbb3d,birdcall-pannsatt-aux-weak:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F782938%2F1346120%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T135616Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dc8a743bedeab7f7ed98053f43adcaaef0825445e89c3ce4edbe0e1c5f4c1e29e2884652058be983146ab28730d79fffd502bc9681fec9bc20dc48d7df7fa485628c3cffc33437d2a8f0f0ebc7c06b8a5267586ccca32727f46f3f4aba007e4857b4e4aaf586d46a9ae52dda15bba2555910f11e2ac65638a29248170d52609573f5dfdd9a49201dd648bef59b52750d942a9fee485ad256e79bb907bb407ffe8b94f3432851b53a5162aa8987b5613cf5d78cc585a646bfb50a60a3972e98d7b6f747bf4b9e2b0435ebca7858550f53e3bc825688d3b17df0f180c49d6aefdcb0bcb93e9b9308cd4e2648966cce50d1978e09ac0b7a55e634d0cf51e69ff7fcb,pannscnn14-decisionlevelatt-weight:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F821504%2F1405119%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T135616Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3e53c6f885b67626a015b7fd4337b70c591c7d6b5ea98af162410695fa4dab1195e641560ace47df3960e5467e21c58f129fa50f22428bd1c6f5c6f55a9e889789ba733e2ff1c9268e4d13131b9cd2edf3c926487f9b91473f8254eeb184c636e3159270f8b4187b8d19214216b0c6c40d5642a0979124b045ec848a39b4291248ca14dbcf130e04dade9694b38b8f901adc32832de847ee5bb134b2b11187b47dbf04ac803ee99f474cc62a4730c34e6bf11ef9acd1f502e6ba8cb1e63180b243ea2b789b5837aff0fbb31c6f19abf203c545ad2f837c5d7ca05de7add1fa456ae92b7c3959027dc672f286421d963c4fb567b0da3fd949b74e6b652387ae58'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "wYWFZ_NvOzOl",
        "outputId": "bc068e6a-49f5-46ef-b84b-ebd8ede9e07f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading birdsong-recognition, 23749499645 bytes compressed\n",
            "[==================================================] 23749499645 bytes downloaded\n",
            "Downloaded and uncompressed: birdsong-recognition\n",
            "Downloading birdcall-check, 16054242 bytes compressed\n",
            "[==================================================] 16054242 bytes downloaded\n",
            "Downloaded and uncompressed: birdcall-check\n",
            "Downloading birdsong-resampled-train-audio-00, 12614447447 bytes compressed\n",
            "[==================================================] 12614447447 bytes downloaded\n",
            "Downloaded and uncompressed: birdsong-resampled-train-audio-00\n",
            "Downloading birdsong-resampled-train-audio-01, 9936971097 bytes compressed\n",
            "[==================================================] 9936971097 bytes downloaded\n",
            "Downloaded and uncompressed: birdsong-resampled-train-audio-01\n",
            "Downloading birdsong-resampled-train-audio-03, 9541386253 bytes compressed\n",
            "[==================================================] 9541386253 bytes downloaded\n",
            "Downloaded and uncompressed: birdsong-resampled-train-audio-03\n",
            "Downloading birdsong-resampled-train-audio-04, 11790849376 bytes compressed\n",
            "[==================================================] 11790849376 bytes downloadedFailed to load https://storage.googleapis.com/kaggle-data-sets/756745/1307127/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240415%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240415T135615Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0ddb95516f1e13c5baa7600fc3702a5c39710325bb09567f9a09faf82edf456d0519c88bbe54b74547ebac13278d45a8a1f817435ef2e171bcbb1e6106d2b582aa16dc8798647886aad3fb188d45d12b585917e262c96879a8b6c0beb140a8b5e8000e8ab838e6bbba67eaf5af3249d5024ed46c4e63f0997246c3ca9d1dfd1a0a0648c7b7254960bcabb2dd1e232b26bd50fbb0da7cd0dceaf3f60aacb04940095d1c8fdc1286565ba6e0adc548b1cabe6bf76e4dea57e22c18b38075cf0a4f20d63bebbbabd343dfb973fac568cec12f0bd98581b3a4f5f0e971e0b029476cbfdd62e36f1b85d9d9ecf08ed5894aad8d28296e30eef18f177e900ed144ee5c to path /kaggle/input/birdsong-resampled-train-audio-04\n",
            "Downloading birdsong-resampled-train-audio-02, 12043219963 bytes compressed\n",
            "[================================================  ] 11790827520 bytes downloadedFailed to load https://storage.googleapis.com/kaggle-data-sets/756953/1307546/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240415%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240415T135615Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=60561dc625780fc2663c07c471dd4bb9f585b77b824fd68829c5533b0df28e00788a9f0a37832173b3e8e8e02c06ed13b3f36c3a1b7f5c3f51db6f911ba16c7dcc4fe43925ea7f35c176585fca2233dd2c9cf4c321b3456de78f2e7f69925380e4504a4097fd28969b891265fe64e6961bbd9939f7c85edaf91962b6ac9b520d98710cc37e9aafaa60025f2edc270853ffe617ede494863b81a58178831f3cd51bd424004220eb738496c0ad70575169452852730e6e21009360313ffb9a80c94a6917a835ce12b1e3060f2665fdc452828893b97bf6ddf21d03063832f8e45d95a0645034a1b9428d2d27b9477554a502e3dfc5f3b5fea899f83cdf05efbb3d to path /kaggle/input/birdsong-resampled-train-audio-02\n",
            "Downloading birdcall-pannsatt-aux-weak, 304440045 bytes compressed\n",
            "[==================================================] 304440045 bytes downloaded\n",
            "Downloaded and uncompressed: birdcall-pannsatt-aux-weak\n",
            "Downloading pannscnn14-decisionlevelatt-weight, 310213474 bytes compressed\n",
            "[==================================================] 310213474 bytes downloaded\n",
            "Downloaded and uncompressed: pannscnn14-decisionlevelatt-weight\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update"
      ],
      "metadata": {
        "id": "TFegpCoIOzOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version note\n",
        "\n",
        "v3:\n",
        "\n",
        "* Training procedure didn't use pretrained weight. Changed to use pretrained weight.\n",
        "* Add link to original paper."
      ],
      "metadata": {
        "id": "Od3N4qdMOzOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About\n",
        "\n",
        "In this notebook, I will introduce Sound Event Detection (SED) task and model fit for that task, and I will show how to train SED model with only *weak* annotation.\n",
        "\n",
        "![SED-overview](http://d33wubrfki0l68.cloudfront.net/508a62f305652e6d9af853c65ab33ae9900ff38e/17a88/images/tasks/challenge2016/task3_overview.png)\n",
        "\n",
        "In SED task, we need to detect sound events from continuous (long) audio clip, and provide prediction of *what sound event exists from when to when*. Therefore our prediction for SED task should look like this.\n",
        "\n",
        "|clip_id|onset|offset|event|\n",
        "|---|---|---|---|\n",
        "|audio_001|0.952|1.87|car|\n",
        "|audio_001|3.82|6.75|speech|\n",
        "|audio_001|5.32|9.28|alarm|\n",
        "\n",
        "<br/>\n",
        "\n",
        "SED task is different from the tasks in past audio competitions in kaggle. The task in [Freesound Audio Tagging 2019](https://www.kaggle.com/c/freesound-audio-tagging-2019) or [Freesound General-Purpose Audio Tagging Challenge](https://www.kaggle.com/c/freesound-audio-tagging) is Audio Tagging, which we'll need to provide clip level prediction, and the task in [TensorFlow Speech Recognition Challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge) is Speech Recognition, so what we need to predict is which speech command is in that audio clip (which is in a sense similar to Audio Tagging task, because we only need to provide clip level prediction).\n",
        "\n",
        "In this competition, what we need to provide is 5sec chunk level prediction for `site_1` and `site_2` data, and clip level prediction for `site_3` data. Chunk level prediction can be treated as audio tagging task if we treat each chunk as short audio clip, but we can also use SED approach."
      ],
      "metadata": {
        "id": "4FwLwBWROzO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model for SED task\n",
        "\n",
        "How can we provide prediction with `onset` and `offset` time information? To do this, models for SED task output *segment-wise* prediction instead of outputting aggregated prediction for a clip, which is usually used for Audio Tagging model.\n",
        "\n",
        "How can we output *segment-wise* prediction? The idea is simple. Assume we use 2D CNN based model which takes log-melspectrogram as input and extract features using CNN feature extractor, and do classification with the feature map which is the output of CNN.\n",
        "\n",
        "The output of CNN feature extractor still contains information about frequency and time(it should be 4 dimensional: (batch size, channels, frequency, time)), so if we aggregate it only in frequency axis, we can preserve time information on that feature map. That feature map has information about which time segment has what sound event.\n",
        "\n",
        "Now that I've introduced the basic idea, let's look into a SED model with some code. In this notebook, I'll use (Weakly-supervised) SED model provided by [PANNs repository](https://github.com/qiuqiangkong/audioset_tagging_cnn/). The model here is pretrained with [AudioSet](https://research.google.com/audioset/), which is an ImageNet counterpart in audio field.\n",
        "\n",
        "[PANNs paper](https://arxiv.org/abs/1912.10211)"
      ],
      "metadata": {
        "id": "e0vT1G0gOzO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import audioread\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import librosa\n",
        "import librosa.display as display\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "from contextlib import contextmanager\n",
        "from IPython.display import Audio\n",
        "from pathlib import Path\n",
        "from typing import Optional, List\n",
        "\n",
        "from catalyst.dl import SupervisedRunner, State, CallbackOrder, Callback, CheckpointCallback\n",
        "from fastprogress import progress_bar\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, average_precision_score"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:46.407504Z",
          "iopub.execute_input": "2024-04-07T12:24:46.407852Z",
          "iopub.status.idle": "2024-04-07T12:24:55.71532Z",
          "shell.execute_reply.started": "2024-04-07T12:24:46.407808Z",
          "shell.execute_reply": "2024-04-07T12:24:55.714534Z"
        },
        "trusted": true,
        "id": "dLKKnRWUOzO3",
        "outputId": "495b27ac-f9ab-4b4d-e71e-a109e452d4b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catalyst'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d2e43275b061>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatalyst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSupervisedRunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallbackOrder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCheckpointCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastprogress\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catalyst'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
        "\n",
        "\n",
        "def get_logger(out_file=None):\n",
        "    logger = logging.getLogger()\n",
        "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "    logger.handlers = []\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    handler = logging.StreamHandler()\n",
        "    handler.setFormatter(formatter)\n",
        "    handler.setLevel(logging.INFO)\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "    if out_file is not None:\n",
        "        fh = logging.FileHandler(out_file)\n",
        "        fh.setFormatter(formatter)\n",
        "        fh.setLevel(logging.INFO)\n",
        "        logger.addHandler(fh)\n",
        "    logger.info(\"logger set up\")\n",
        "    return logger\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def timer(name: str, logger: Optional[logging.Logger] = None):\n",
        "    t0 = time.time()\n",
        "    msg = f\"[{name}] start\"\n",
        "    if logger is None:\n",
        "        print(msg)\n",
        "    else:\n",
        "        logger.info(msg)\n",
        "    yield\n",
        "\n",
        "    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n",
        "    if logger is None:\n",
        "        print(msg)\n",
        "    else:\n",
        "        logger.info(msg)\n",
        "\n",
        "\n",
        "set_seed(1213)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:55.717416Z",
          "iopub.execute_input": "2024-04-07T12:24:55.717691Z",
          "iopub.status.idle": "2024-04-07T12:24:55.734993Z",
          "shell.execute_reply.started": "2024-04-07T12:24:55.717664Z",
          "shell.execute_reply": "2024-04-07T12:24:55.7343Z"
        },
        "trusted": true,
        "id": "QiFsuHqIOzO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd().parent\n",
        "INPUT_ROOT = ROOT / \"input\"\n",
        "RAW_DATA = INPUT_ROOT / \"birdsong-recognition\"\n",
        "TRAIN_AUDIO_DIR = RAW_DATA / \"train_audio\"\n",
        "TRAIN_RESAMPLED_AUDIO_DIRS = [\n",
        "  INPUT_ROOT / \"birdsong-resampled-train-audio-{:0>2}\".format(i)  for i in range(5)\n",
        "]\n",
        "TEST_AUDIO_DIR = RAW_DATA / \"test_audio\""
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:55.736315Z",
          "iopub.execute_input": "2024-04-07T12:24:55.736565Z",
          "iopub.status.idle": "2024-04-07T12:24:55.749555Z",
          "shell.execute_reply.started": "2024-04-07T12:24:55.73654Z",
          "shell.execute_reply": "2024-04-07T12:24:55.748868Z"
        },
        "trusted": true,
        "id": "56U-3VLOOzO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(TRAIN_RESAMPLED_AUDIO_DIRS[0] / \"train_mod.csv\")\n",
        "\n",
        "if not TEST_AUDIO_DIR.exists():\n",
        "    TEST_AUDIO_DIR = INPUT_ROOT / \"birdcall-check\" / \"test_audio\"\n",
        "    test = pd.read_csv(INPUT_ROOT / \"birdcall-check\" / \"test.csv\")\n",
        "else:\n",
        "    test = pd.read_csv(RAW_DATA / \"test.csv\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:55.750768Z",
          "iopub.execute_input": "2024-04-07T12:24:55.751152Z",
          "iopub.status.idle": "2024-04-07T12:24:56.233012Z",
          "shell.execute_reply.started": "2024-04-07T12:24:55.751092Z",
          "shell.execute_reply": "2024-04-07T12:24:56.232363Z"
        },
        "trusted": true,
        "id": "t8TAweYMOzO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torchlibrosa\n",
        "\n",
        "\n",
        "In PANNs, `torchlibrosa`, a PyTorch based implementation are used to replace some of the `librosa`'s functions. Here I use some functions of `torchlibrosa`.\n",
        "\n",
        "Ref: https://github.com/qiuqiangkong/torchlibrosa"
      ],
      "metadata": {
        "id": "hF-YYSBiOzO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LICENSE\n",
        "\n",
        "```\n",
        "ISC License\n",
        "Copyright (c) 2013--2017, librosa development team.\n",
        "\n",
        "Permission to use, copy, modify, and/or distribute this software for any purpose with or without fee is hereby granted, provided that the above copyright notice and this permission notice appear in all copies.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n",
        "```"
      ],
      "metadata": {
        "id": "SZzrN_saOzO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DFTBase(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Base class for DFT and IDFT matrix\"\"\"\n",
        "        super(DFTBase, self).__init__()\n",
        "\n",
        "    def dft_matrix(self, n):\n",
        "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
        "        omega = np.exp(-2 * np.pi * 1j / n)\n",
        "        W = np.power(omega, x * y)\n",
        "        return W\n",
        "\n",
        "    def idft_matrix(self, n):\n",
        "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
        "        omega = np.exp(2 * np.pi * 1j / n)\n",
        "        W = np.power(omega, x * y)\n",
        "        return W\n",
        "\n",
        "\n",
        "class STFT(DFTBase):\n",
        "    def __init__(self, n_fft=2048, hop_length=None, win_length=None,\n",
        "        window='hann', center=True, pad_mode='reflect', freeze_parameters=True):\n",
        "        \"\"\"Implementation of STFT with Conv1d. The function has the same output\n",
        "        of librosa.core.stft\n",
        "        \"\"\"\n",
        "        super(STFT, self).__init__()\n",
        "\n",
        "        assert pad_mode in ['constant', 'reflect']\n",
        "\n",
        "        self.n_fft = n_fft\n",
        "        self.center = center\n",
        "        self.pad_mode = pad_mode\n",
        "\n",
        "        # By default, use the entire frame\n",
        "        if win_length is None:\n",
        "            win_length = n_fft\n",
        "\n",
        "        # Set the default hop, if it's not already specified\n",
        "        if hop_length is None:\n",
        "            hop_length = int(win_length // 4)\n",
        "\n",
        "        fft_window = librosa.filters.get_window(window, win_length, fftbins=True)\n",
        "\n",
        "        # Pad the window out to n_fft size\n",
        "        fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
        "\n",
        "        # DFT & IDFT matrix\n",
        "        self.W = self.dft_matrix(n_fft)\n",
        "\n",
        "        out_channels = n_fft // 2 + 1\n",
        "\n",
        "        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels,\n",
        "            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1,\n",
        "            groups=1, bias=False)\n",
        "\n",
        "        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels,\n",
        "            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1,\n",
        "            groups=1, bias=False)\n",
        "\n",
        "        self.conv_real.weight.data = torch.Tensor(\n",
        "            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
        "        # (n_fft // 2 + 1, 1, n_fft)\n",
        "\n",
        "        self.conv_imag.weight.data = torch.Tensor(\n",
        "            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
        "        # (n_fft // 2 + 1, 1, n_fft)\n",
        "\n",
        "        if freeze_parameters:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"input: (batch_size, data_length)\n",
        "        Returns:\n",
        "          real: (batch_size, n_fft // 2 + 1, time_steps)\n",
        "          imag: (batch_size, n_fft // 2 + 1, time_steps)\n",
        "        \"\"\"\n",
        "\n",
        "        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n",
        "\n",
        "        if self.center:\n",
        "            x = F.pad(x, pad=(self.n_fft // 2, self.n_fft // 2), mode=self.pad_mode)\n",
        "\n",
        "        real = self.conv_real(x)\n",
        "        imag = self.conv_imag(x)\n",
        "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
        "\n",
        "        real = real[:, None, :, :].transpose(2, 3)\n",
        "        imag = imag[:, None, :, :].transpose(2, 3)\n",
        "        # (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
        "\n",
        "        return real, imag\n",
        "\n",
        "\n",
        "class Spectrogram(nn.Module):\n",
        "    def __init__(self, n_fft=2048, hop_length=None, win_length=None,\n",
        "        window='hann', center=True, pad_mode='reflect', power=2.0,\n",
        "        freeze_parameters=True):\n",
        "        \"\"\"Calculate spectrogram using pytorch. The STFT is implemented with\n",
        "        Conv1d. The function has the same output of librosa.core.stft\n",
        "        \"\"\"\n",
        "        super(Spectrogram, self).__init__()\n",
        "\n",
        "        self.power = power\n",
        "\n",
        "        self.stft = STFT(n_fft=n_fft, hop_length=hop_length,\n",
        "            win_length=win_length, window=window, center=center,\n",
        "            pad_mode=pad_mode, freeze_parameters=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"input: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
        "        Returns:\n",
        "          spectrogram: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
        "        \"\"\"\n",
        "\n",
        "        (real, imag) = self.stft.forward(input)\n",
        "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
        "\n",
        "        spectrogram = real ** 2 + imag ** 2\n",
        "\n",
        "        if self.power == 2.0:\n",
        "            pass\n",
        "        else:\n",
        "            spectrogram = spectrogram ** (power / 2.0)\n",
        "\n",
        "        return spectrogram\n",
        "\n",
        "\n",
        "class LogmelFilterBank(nn.Module):\n",
        "    def __init__(self, sr=32000, n_fft=2048, n_mels=64, fmin=50, fmax=14000, is_log=True,\n",
        "        ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n",
        "        \"\"\"Calculate logmel spectrogram using pytorch. The mel filter bank is\n",
        "        the pytorch implementation of as librosa.filters.mel\n",
        "        \"\"\"\n",
        "        super(LogmelFilterBank, self).__init__()\n",
        "\n",
        "        self.is_log = is_log\n",
        "        self.ref = ref\n",
        "        self.amin = amin\n",
        "        self.top_db = top_db\n",
        "\n",
        "        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n",
        "            fmin=fmin, fmax=fmax).T\n",
        "        # (n_fft // 2 + 1, mel_bins)\n",
        "\n",
        "        self.melW = nn.Parameter(torch.Tensor(self.melW))\n",
        "\n",
        "        if freeze_parameters:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"input: (batch_size, channels, time_steps)\n",
        "\n",
        "        Output: (batch_size, time_steps, mel_bins)\n",
        "        \"\"\"\n",
        "\n",
        "        # Mel spectrogram\n",
        "        mel_spectrogram = torch.matmul(input, self.melW)\n",
        "\n",
        "        # Logmel spectrogram\n",
        "        if self.is_log:\n",
        "            output = self.power_to_db(mel_spectrogram)\n",
        "        else:\n",
        "            output = mel_spectrogram\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def power_to_db(self, input):\n",
        "        \"\"\"Power to db, this function is the pytorch implementation of\n",
        "        librosa.core.power_to_lb\n",
        "        \"\"\"\n",
        "        ref_value = self.ref\n",
        "        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n",
        "        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n",
        "\n",
        "        if self.top_db is not None:\n",
        "            if self.top_db < 0:\n",
        "                raise ParameterError('top_db must be non-negative')\n",
        "            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n",
        "\n",
        "        return log_spec"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:56.236094Z",
          "iopub.execute_input": "2024-04-07T12:24:56.236404Z",
          "iopub.status.idle": "2024-04-07T12:24:56.282067Z",
          "shell.execute_reply.started": "2024-04-07T12:24:56.236375Z",
          "shell.execute_reply": "2024-04-07T12:24:56.281209Z"
        },
        "trusted": true,
        "id": "7fmrlOmoOzO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DropStripes(nn.Module):\n",
        "    def __init__(self, dim, drop_width, stripes_num):\n",
        "        \"\"\"Drop stripes.\n",
        "        Args:\n",
        "          dim: int, dimension along which to drop\n",
        "          drop_width: int, maximum width of stripes to drop\n",
        "          stripes_num: int, how many stripes to drop\n",
        "        \"\"\"\n",
        "        super(DropStripes, self).__init__()\n",
        "\n",
        "        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n",
        "\n",
        "        self.dim = dim\n",
        "        self.drop_width = drop_width\n",
        "        self.stripes_num = stripes_num\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n",
        "\n",
        "        assert input.ndimension() == 4\n",
        "\n",
        "        if self.training is False:\n",
        "            return input\n",
        "\n",
        "        else:\n",
        "            batch_size = input.shape[0]\n",
        "            total_width = input.shape[self.dim]\n",
        "\n",
        "            for n in range(batch_size):\n",
        "                self.transform_slice(input[n], total_width)\n",
        "\n",
        "            return input\n",
        "\n",
        "\n",
        "    def transform_slice(self, e, total_width):\n",
        "        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n",
        "\n",
        "        for _ in range(self.stripes_num):\n",
        "            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]\n",
        "            bgn = torch.randint(low=0, high=total_width - distance, size=(1,))[0]\n",
        "\n",
        "            if self.dim == 2:\n",
        "                e[:, bgn : bgn + distance, :] = 0\n",
        "            elif self.dim == 3:\n",
        "                e[:, :, bgn : bgn + distance] = 0\n",
        "\n",
        "\n",
        "class SpecAugmentation(nn.Module):\n",
        "    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width,\n",
        "        freq_stripes_num):\n",
        "        \"\"\"Spec augmetation.\n",
        "        [ref] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D.\n",
        "        and Le, Q.V., 2019. Specaugment: A simple data augmentation method\n",
        "        for automatic speech recognition. arXiv preprint arXiv:1904.08779.\n",
        "        Args:\n",
        "          time_drop_width: int\n",
        "          time_stripes_num: int\n",
        "          freq_drop_width: int\n",
        "          freq_stripes_num: int\n",
        "        \"\"\"\n",
        "\n",
        "        super(SpecAugmentation, self).__init__()\n",
        "\n",
        "        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width,\n",
        "            stripes_num=time_stripes_num)\n",
        "\n",
        "        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width,\n",
        "            stripes_num=freq_stripes_num)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.time_dropper(input)\n",
        "        x = self.freq_dropper(x)\n",
        "        return x"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:56.285825Z",
          "iopub.execute_input": "2024-04-07T12:24:56.286085Z",
          "iopub.status.idle": "2024-04-07T12:24:56.303785Z",
          "shell.execute_reply.started": "2024-04-07T12:24:56.286059Z",
          "shell.execute_reply": "2024-04-07T12:24:56.302882Z"
        },
        "trusted": true,
        "id": "TAJLiq5dOzPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### audioset_tagging_cnn\n",
        "\n",
        "I also use `Cnn14_DecisionLevelAtt` model from [PANNs models](https://github.com/qiuqiangkong/audioset_tagging_cnn/blob/master/pytorch/models.py), which is a SED model."
      ],
      "metadata": {
        "id": "JWSryDF4OzPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LICENSE\n",
        "\n",
        "```\n",
        "The MIT License\n",
        "  \n",
        "Copyright (c) 2010-2017 Google, Inc. http://angularjs.org\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in\n",
        "all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "THE SOFTWARE.\n",
        "```"
      ],
      "metadata": {
        "id": "gYCEAavBOzPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building blocks"
      ],
      "metadata": {
        "id": "lXNI1YyLOzPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_layer(layer):\n",
        "    nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "    if hasattr(layer, \"bias\"):\n",
        "        if layer.bias is not None:\n",
        "            layer.bias.data.fill_(0.)\n",
        "\n",
        "\n",
        "def init_bn(bn):\n",
        "    bn.bias.data.fill_(0.)\n",
        "    bn.weight.data.fill_(1.0)\n",
        "\n",
        "\n",
        "def interpolate(x: torch.Tensor, ratio: int):\n",
        "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
        "    resolution reduction in downsampling of a CNN.\n",
        "\n",
        "    Args:\n",
        "      x: (batch_size, time_steps, classes_num)\n",
        "      ratio: int, ratio to interpolate\n",
        "    Returns:\n",
        "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
        "    \"\"\"\n",
        "    (batch_size, time_steps, classes_num) = x.shape\n",
        "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
        "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
        "    return upsampled\n",
        "\n",
        "\n",
        "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
        "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
        "    is the same as the value of the last frame.\n",
        "    Args:\n",
        "      framewise_output: (batch_size, frames_num, classes_num)\n",
        "      frames_num: int, number of frames to pad\n",
        "    Outputs:\n",
        "      output: (batch_size, frames_num, classes_num)\n",
        "    \"\"\"\n",
        "    pad = framewise_output[:, -1:, :].repeat(\n",
        "        1, frames_num - framewise_output.shape[1], 1)\n",
        "    \"\"\"tensor for padding\"\"\"\n",
        "\n",
        "    output = torch.cat((framewise_output, pad), dim=1)\n",
        "    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1),\n",
        "            bias=False)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=out_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1),\n",
        "            bias=False)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        init_layer(self.conv1)\n",
        "        init_layer(self.conv2)\n",
        "        init_bn(self.bn1)\n",
        "        init_bn(self.bn2)\n",
        "\n",
        "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
        "\n",
        "        x = input\n",
        "        x = F.relu_(self.bn1(self.conv1(x)))\n",
        "        x = F.relu_(self.bn2(self.conv2(x)))\n",
        "        if pool_type == 'max':\n",
        "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
        "        elif pool_type == 'avg':\n",
        "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
        "        elif pool_type == 'avg+max':\n",
        "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
        "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
        "            x = x1 + x2\n",
        "        else:\n",
        "            raise Exception('Incorrect argument!')\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features: int,\n",
        "                 out_features: int,\n",
        "                 activation=\"linear\",\n",
        "                 temperature=1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.activation = activation\n",
        "        self.temperature = temperature\n",
        "        self.att = nn.Conv1d(\n",
        "            in_channels=in_features,\n",
        "            out_channels=out_features,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=True)\n",
        "        self.cla = nn.Conv1d(\n",
        "            in_channels=in_features,\n",
        "            out_channels=out_features,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=True)\n",
        "\n",
        "        self.bn_att = nn.BatchNorm1d(out_features)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_layer(self.att)\n",
        "        init_layer(self.cla)\n",
        "        init_bn(self.bn_att)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (n_samples, n_in, n_time)\n",
        "        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n",
        "        cla = self.nonlinear_transform(self.cla(x))\n",
        "        x = torch.sum(norm_att * cla, dim=2)\n",
        "        return x, norm_att, cla\n",
        "\n",
        "    def nonlinear_transform(self, x):\n",
        "        if self.activation == 'linear':\n",
        "            return x\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return torch.sigmoid(x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:56.305304Z",
          "iopub.execute_input": "2024-04-07T12:24:56.305628Z",
          "iopub.status.idle": "2024-04-07T12:24:56.339518Z",
          "shell.execute_reply.started": "2024-04-07T12:24:56.305599Z",
          "shell.execute_reply": "2024-04-07T12:24:56.338523Z"
        },
        "trusted": true,
        "id": "VVgtE610OzPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PANNsCNN14Att(nn.Module):\n",
        "    def __init__(self, sample_rate: int, window_size: int, hop_size: int,\n",
        "                 mel_bins: int, fmin: int, fmax: int, classes_num: int):\n",
        "        super().__init__()\n",
        "\n",
        "        window = 'hann'\n",
        "        center = True\n",
        "        pad_mode = 'reflect'\n",
        "        ref = 1.0\n",
        "        amin = 1e-10\n",
        "        top_db = None\n",
        "        self.interpolate_ratio = 32  # Downsampled ratio\n",
        "\n",
        "        # Spectrogram extractor\n",
        "        self.spectrogram_extractor = Spectrogram(\n",
        "            n_fft=window_size,\n",
        "            hop_length=hop_size,\n",
        "            win_length=window_size,\n",
        "            window=window,\n",
        "            center=center,\n",
        "            pad_mode=pad_mode,\n",
        "            freeze_parameters=True)\n",
        "\n",
        "        # Logmel feature extractor\n",
        "        self.logmel_extractor = LogmelFilterBank(\n",
        "            sr=sample_rate,\n",
        "            n_fft=window_size,\n",
        "            n_mels=mel_bins,\n",
        "            fmin=fmin,\n",
        "            fmax=fmax,\n",
        "            ref=ref,\n",
        "            amin=amin,\n",
        "            top_db=top_db,\n",
        "            freeze_parameters=True)\n",
        "\n",
        "        # Spec augmenter\n",
        "        self.spec_augmenter = SpecAugmentation(\n",
        "            time_drop_width=64,\n",
        "            time_stripes_num=2,\n",
        "            freq_drop_width=8,\n",
        "            freq_stripes_num=2)\n",
        "\n",
        "        self.bn0 = nn.BatchNorm2d(mel_bins)\n",
        "\n",
        "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
        "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
        "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
        "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
        "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
        "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
        "\n",
        "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
        "        self.att_block = AttBlock(2048, classes_num, activation='sigmoid')\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        init_bn(self.bn0)\n",
        "        init_layer(self.fc1)\n",
        "\n",
        "    def cnn_feature_extractor(self, x):\n",
        "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        return x\n",
        "\n",
        "    def preprocess(self, input, mixup_lambda=None):\n",
        "        # t1 = time.time()\n",
        "        x = self.spectrogram_extractor(input)  # (batch_size, 1, time_steps, freq_bins)\n",
        "        x = self.logmel_extractor(x)  # (batch_size, 1, time_steps, mel_bins)\n",
        "\n",
        "        frames_num = x.shape[2]\n",
        "\n",
        "        x = x.transpose(1, 3)\n",
        "        x = self.bn0(x)\n",
        "        x = x.transpose(1, 3)\n",
        "\n",
        "        if self.training:\n",
        "            x = self.spec_augmenter(x)\n",
        "\n",
        "        # Mixup on spectrogram\n",
        "        if self.training and mixup_lambda is not None:\n",
        "            x = do_mixup(x, mixup_lambda)\n",
        "        return x, frames_num\n",
        "\n",
        "\n",
        "    def forward(self, input, mixup_lambda=None):\n",
        "        \"\"\"\n",
        "        Input: (batch_size, data_length)\"\"\"\n",
        "        x, frames_num = self.preprocess(input, mixup_lambda=mixup_lambda)\n",
        "\n",
        "        # Output shape (batch size, channels, time, frequency)\n",
        "        x = self.cnn_feature_extractor(x)\n",
        "\n",
        "        # Aggregate in frequency axis\n",
        "        x = torch.mean(x, dim=3)\n",
        "\n",
        "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
        "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
        "        x = x1 + x2\n",
        "\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = F.relu_(self.fc1(x))\n",
        "        x = x.transpose(1, 2)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
        "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
        "\n",
        "        # Get framewise output\n",
        "        framewise_output = interpolate(segmentwise_output,\n",
        "                                       self.interpolate_ratio)\n",
        "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
        "\n",
        "        output_dict = {\n",
        "            'framewise_output': framewise_output,\n",
        "            'clipwise_output': clipwise_output\n",
        "        }\n",
        "\n",
        "        return output_dict"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:56.340968Z",
          "iopub.execute_input": "2024-04-07T12:24:56.341354Z",
          "iopub.status.idle": "2024-04-07T12:24:56.377028Z",
          "shell.execute_reply.started": "2024-04-07T12:24:56.341315Z",
          "shell.execute_reply": "2024-04-07T12:24:56.376041Z"
        },
        "trusted": true,
        "id": "grAPiEtZOzPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is good in PANNs models is that they accept raw audio clip as input. Let's put a chunk into the CNN feature extractor of the model above."
      ],
      "metadata": {
        "id": "ZnVyTuHlOzPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SR = 32000\n",
        "\n",
        "y, _ = librosa.load(TRAIN_RESAMPLED_AUDIO_DIRS[0] / \"aldfly\" / \"XC134874.wav\",\n",
        "                    sr=SR,\n",
        "                    res_type=\"kaiser_fast\",\n",
        "                    mono=True)\n",
        "\n",
        "Audio(y, rate=SR)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:56.37835Z",
          "iopub.execute_input": "2024-04-07T12:24:56.378697Z",
          "iopub.status.idle": "2024-04-07T12:24:56.495021Z",
          "shell.execute_reply.started": "2024-04-07T12:24:56.378667Z",
          "shell.execute_reply": "2024-04-07T12:24:56.493811Z"
        },
        "trusted": true,
        "id": "iffBx2AmOzPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display.waveplot(y, sr=SR);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:56.49677Z",
          "iopub.execute_input": "2024-04-07T12:24:56.497138Z",
          "iopub.status.idle": "2024-04-07T12:24:56.678995Z",
          "shell.execute_reply.started": "2024-04-07T12:24:56.497088Z",
          "shell.execute_reply": "2024-04-07T12:24:56.678223Z"
        },
        "trusted": true,
        "id": "JoOjkJk7OzPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    \"sample_rate\": 32000,\n",
        "    \"window_size\": 1024,\n",
        "    \"hop_size\": 320,\n",
        "    \"mel_bins\": 64,\n",
        "    \"fmin\": 50,\n",
        "    \"fmax\": 14000,\n",
        "    \"classes_num\": 264\n",
        "}\n",
        "\n",
        "model = PANNsCNN14Att(**model_config)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:56.680681Z",
          "iopub.execute_input": "2024-04-07T12:24:56.681066Z",
          "iopub.status.idle": "2024-04-07T12:24:58.128774Z",
          "shell.execute_reply.started": "2024-04-07T12:24:56.681025Z",
          "shell.execute_reply": "2024-04-07T12:24:58.1279Z"
        },
        "trusted": true,
        "id": "gcKHCLBxOzPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In `PANNsCNN14Att`, input raw waveform will be converted into log-melspectrogram using `torchlibrosa`'s utilities. I put this functionality in `PANNsCNN14Att.preprocess()` method. Let's check the output."
      ],
      "metadata": {
        "id": "mvhliKBbOzPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk = torch.from_numpy(y[:SR * 5]).unsqueeze(0)\n",
        "melspec, _ = model.preprocess(chunk)\n",
        "melspec.size()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:58.13036Z",
          "iopub.execute_input": "2024-04-07T12:24:58.130677Z",
          "iopub.status.idle": "2024-04-07T12:24:58.219844Z",
          "shell.execute_reply.started": "2024-04-07T12:24:58.13064Z",
          "shell.execute_reply": "2024-04-07T12:24:58.218916Z"
        },
        "trusted": true,
        "id": "zlq27-thOzPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melspec_numpy = melspec.detach().numpy()[0, 0].transpose(1, 0)\n",
        "display.specshow(melspec_numpy, sr=SR, y_axis=\"mel\");"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:58.221481Z",
          "iopub.execute_input": "2024-04-07T12:24:58.221934Z",
          "iopub.status.idle": "2024-04-07T12:24:58.36605Z",
          "shell.execute_reply.started": "2024-04-07T12:24:58.221888Z",
          "shell.execute_reply": "2024-04-07T12:24:58.365179Z"
        },
        "trusted": true,
        "id": "oRHZ-x1XOzPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`PANNsCNN14Att.cnn_feature_extractor()` method will take this as input and output feature map. Let's check the output of the feature extractor."
      ],
      "metadata": {
        "id": "XHDyYerSOzPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map = model.cnn_feature_extractor(melspec)\n",
        "feature_map.size()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:24:58.36769Z",
          "iopub.execute_input": "2024-04-07T12:24:58.368087Z",
          "iopub.status.idle": "2024-04-07T12:24:58.733432Z",
          "shell.execute_reply.started": "2024-04-07T12:24:58.368046Z",
          "shell.execute_reply": "2024-04-07T12:24:58.732576Z"
        },
        "trusted": true,
        "id": "6Y5sTkBLOzPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although it's downsized through several convolution and pooling layers, the size of it's third dimension is 15 and it still contains time information. Each element of this dimension is *segment*. In SED model, we provide prediction for each of this."
      ],
      "metadata": {
        "id": "T0xKbkLXOzPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train SED model with only weak supervision\n",
        "\n",
        "![weak-label-vs-strong-label](https://www.researchgate.net/profile/Anurag_Kumar10/publication/329239818/figure/fig5/AS:743089203322880@1554177680169/Weakly-Labeled-vs-Strongly-Labeled-Strongly-labeled-data-contains-time-stamps-of-the.ppm)\n",
        "\n",
        "<br/>\n",
        "\n",
        "This figure gives us an intuitive explanation what is *weak annotation* and what is *strong annotation* in terms of sound event detection. For this competition, we only have weak annotation (clip level annotation). Therefore, we need to train our SED model in weakly-supervised manner.\n",
        "\n",
        "In weakly-supervised setting, we only have clip-level annotation, therefore we also need to aggregate that in time axis. Hense, we at first put classifier that outputs class existence probability for each time step just after the feature extractor and then aggregate the output of the classifier result in time axis.\n",
        "In this way we can get both clip-level prediction and segment-level prediction (if the time resolution is high, it can be treated as event-level prediction). Then we train it normally by using BCE loss with clip-level prediction and clip-level annotation.\n",
        "\n",
        "Let's check how this is implemented in the PANNs model above. segment-wise prediction and clip-wise prediction is actually calculated in `AttBlock` of the model.\n",
        "\n",
        "```\n",
        "class AttBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features: int,\n",
        "                 out_features: int,\n",
        "                 activation=\"linear\",\n",
        "                 temperature=1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.activation = activation\n",
        "        self.temperature = temperature\n",
        "        self.att = nn.Conv1d(\n",
        "            in_channels=in_features,\n",
        "            out_channels=out_features,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=True)\n",
        "        self.cla = nn.Conv1d(\n",
        "            in_channels=in_features,\n",
        "            out_channels=out_features,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=True)\n",
        "\n",
        "        self.bn_att = nn.BatchNorm1d(out_features)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_layer(self.att)\n",
        "        init_layer(self.cla)\n",
        "        init_bn(self.bn_att)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (n_samples, n_in, n_time)\n",
        "        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n",
        "        cla = self.nonlinear_transform(self.cla(x))\n",
        "        x = torch.sum(norm_att * cla, dim=2)\n",
        "        return x, norm_att, cla\n",
        "\n",
        "    def nonlinear_transform(self, x):\n",
        "        if self.activation == 'linear':\n",
        "            return x\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return torch.sigmoid(x)\n",
        "```"
      ],
      "metadata": {
        "id": "Mhr0fsmPOzPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the `forward` method, it at first calculate self-attention map in the first line `norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)`. This will be used to aggregate the classification result for segment. In the second line, `cla = self.nonlinear_transform(self.cla(x))` calculates segment wise classification result. Then in the third line, attention aggregation is performed to get clip wise prediction."
      ],
      "metadata": {
        "id": "hfoTxU7POzPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try to train this model in weakly-supervised manner."
      ],
      "metadata": {
        "id": "-bFLbsl9OzPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "PNHsjX0jOzPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BIRD_CODE = {\n",
        "    'aldfly': 0, 'ameavo': 1, 'amebit': 2, 'amecro': 3, 'amegfi': 4,\n",
        "    'amekes': 5, 'amepip': 6, 'amered': 7, 'amerob': 8, 'amewig': 9,\n",
        "    'amewoo': 10, 'amtspa': 11, 'annhum': 12, 'astfly': 13, 'baisan': 14,\n",
        "    'baleag': 15, 'balori': 16, 'banswa': 17, 'barswa': 18, 'bawwar': 19,\n",
        "    'belkin1': 20, 'belspa2': 21, 'bewwre': 22, 'bkbcuc': 23, 'bkbmag1': 24,\n",
        "    'bkbwar': 25, 'bkcchi': 26, 'bkchum': 27, 'bkhgro': 28, 'bkpwar': 29,\n",
        "    'bktspa': 30, 'blkpho': 31, 'blugrb1': 32, 'blujay': 33, 'bnhcow': 34,\n",
        "    'boboli': 35, 'bongul': 36, 'brdowl': 37, 'brebla': 38, 'brespa': 39,\n",
        "    'brncre': 40, 'brnthr': 41, 'brthum': 42, 'brwhaw': 43, 'btbwar': 44,\n",
        "    'btnwar': 45, 'btywar': 46, 'buffle': 47, 'buggna': 48, 'buhvir': 49,\n",
        "    'bulori': 50, 'bushti': 51, 'buwtea': 52, 'buwwar': 53, 'cacwre': 54,\n",
        "    'calgul': 55, 'calqua': 56, 'camwar': 57, 'cangoo': 58, 'canwar': 59,\n",
        "    'canwre': 60, 'carwre': 61, 'casfin': 62, 'caster1': 63, 'casvir': 64,\n",
        "    'cedwax': 65, 'chispa': 66, 'chiswi': 67, 'chswar': 68, 'chukar': 69,\n",
        "    'clanut': 70, 'cliswa': 71, 'comgol': 72, 'comgra': 73, 'comloo': 74,\n",
        "    'commer': 75, 'comnig': 76, 'comrav': 77, 'comred': 78, 'comter': 79,\n",
        "    'comyel': 80, 'coohaw': 81, 'coshum': 82, 'cowscj1': 83, 'daejun': 84,\n",
        "    'doccor': 85, 'dowwoo': 86, 'dusfly': 87, 'eargre': 88, 'easblu': 89,\n",
        "    'easkin': 90, 'easmea': 91, 'easpho': 92, 'eastow': 93, 'eawpew': 94,\n",
        "    'eucdov': 95, 'eursta': 96, 'evegro': 97, 'fiespa': 98, 'fiscro': 99,\n",
        "    'foxspa': 100, 'gadwal': 101, 'gcrfin': 102, 'gnttow': 103, 'gnwtea': 104,\n",
        "    'gockin': 105, 'gocspa': 106, 'goleag': 107, 'grbher3': 108, 'grcfly': 109,\n",
        "    'greegr': 110, 'greroa': 111, 'greyel': 112, 'grhowl': 113, 'grnher': 114,\n",
        "    'grtgra': 115, 'grycat': 116, 'gryfly': 117, 'haiwoo': 118, 'hamfly': 119,\n",
        "    'hergul': 120, 'herthr': 121, 'hoomer': 122, 'hoowar': 123, 'horgre': 124,\n",
        "    'horlar': 125, 'houfin': 126, 'houspa': 127, 'houwre': 128, 'indbun': 129,\n",
        "    'juntit1': 130, 'killde': 131, 'labwoo': 132, 'larspa': 133, 'lazbun': 134,\n",
        "    'leabit': 135, 'leafly': 136, 'leasan': 137, 'lecthr': 138, 'lesgol': 139,\n",
        "    'lesnig': 140, 'lesyel': 141, 'lewwoo': 142, 'linspa': 143, 'lobcur': 144,\n",
        "    'lobdow': 145, 'logshr': 146, 'lotduc': 147, 'louwat': 148, 'macwar': 149,\n",
        "    'magwar': 150, 'mallar3': 151, 'marwre': 152, 'merlin': 153, 'moublu': 154,\n",
        "    'mouchi': 155, 'moudov': 156, 'norcar': 157, 'norfli': 158, 'norhar2': 159,\n",
        "    'normoc': 160, 'norpar': 161, 'norpin': 162, 'norsho': 163, 'norwat': 164,\n",
        "    'nrwswa': 165, 'nutwoo': 166, 'olsfly': 167, 'orcwar': 168, 'osprey': 169,\n",
        "    'ovenbi1': 170, 'palwar': 171, 'pasfly': 172, 'pecsan': 173, 'perfal': 174,\n",
        "    'phaino': 175, 'pibgre': 176, 'pilwoo': 177, 'pingro': 178, 'pinjay': 179,\n",
        "    'pinsis': 180, 'pinwar': 181, 'plsvir': 182, 'prawar': 183, 'purfin': 184,\n",
        "    'pygnut': 185, 'rebmer': 186, 'rebnut': 187, 'rebsap': 188, 'rebwoo': 189,\n",
        "    'redcro': 190, 'redhea': 191, 'reevir1': 192, 'renpha': 193, 'reshaw': 194,\n",
        "    'rethaw': 195, 'rewbla': 196, 'ribgul': 197, 'rinduc': 198, 'robgro': 199,\n",
        "    'rocpig': 200, 'rocwre': 201, 'rthhum': 202, 'ruckin': 203, 'rudduc': 204,\n",
        "    'rufgro': 205, 'rufhum': 206, 'rusbla': 207, 'sagspa1': 208, 'sagthr': 209,\n",
        "    'savspa': 210, 'saypho': 211, 'scatan': 212, 'scoori': 213, 'semplo': 214,\n",
        "    'semsan': 215, 'sheowl': 216, 'shshaw': 217, 'snobun': 218, 'snogoo': 219,\n",
        "    'solsan': 220, 'sonspa': 221, 'sora': 222, 'sposan': 223, 'spotow': 224,\n",
        "    'stejay': 225, 'swahaw': 226, 'swaspa': 227, 'swathr': 228, 'treswa': 229,\n",
        "    'truswa': 230, 'tuftit': 231, 'tunswa': 232, 'veery': 233, 'vesspa': 234,\n",
        "    'vigswa': 235, 'warvir': 236, 'wesblu': 237, 'wesgre': 238, 'weskin': 239,\n",
        "    'wesmea': 240, 'wessan': 241, 'westan': 242, 'wewpew': 243, 'whbnut': 244,\n",
        "    'whcspa': 245, 'whfibi': 246, 'whtspa': 247, 'whtswi': 248, 'wilfly': 249,\n",
        "    'wilsni1': 250, 'wiltur': 251, 'winwre3': 252, 'wlswar': 253, 'wooduc': 254,\n",
        "    'wooscj2': 255, 'woothr': 256, 'y00475': 257, 'yebfly': 258, 'yebsap': 259,\n",
        "    'yehbla': 260, 'yelwar': 261, 'yerwar': 262, 'yetvir': 263\n",
        "}\n",
        "\n",
        "INV_BIRD_CODE = {v: k for k, v in BIRD_CODE.items()}"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-04-07T12:26:41.450081Z",
          "iopub.execute_input": "2024-04-07T12:26:41.450426Z",
          "iopub.status.idle": "2024-04-07T12:26:41.490077Z",
          "shell.execute_reply.started": "2024-04-07T12:26:41.450397Z",
          "shell.execute_reply": "2024-04-07T12:26:41.489224Z"
        },
        "trusted": true,
        "id": "RcO5yWswOzPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PERIOD = 5\n",
        "\n",
        "class PANNsDataset(data.Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            file_list: List[List[str]],\n",
        "            waveform_transforms=None):\n",
        "        self.file_list = file_list  # list of list: [file_path, ebird_code]\n",
        "        self.waveform_transforms = waveform_transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        wav_path, ebird_code = self.file_list[idx]\n",
        "\n",
        "        y, sr = sf.read(wav_path)\n",
        "\n",
        "        if self.waveform_transforms:\n",
        "            y = self.waveform_transforms(y)\n",
        "        else:\n",
        "            len_y = len(y)\n",
        "            effective_length = sr * PERIOD\n",
        "            if len_y < effective_length:\n",
        "                new_y = np.zeros(effective_length, dtype=y.dtype)\n",
        "                start = np.random.randint(effective_length - len_y)\n",
        "                new_y[start:start + len_y] = y\n",
        "                y = new_y.astype(np.float32)\n",
        "            elif len_y > effective_length:\n",
        "                start = np.random.randint(len_y - effective_length)\n",
        "                y = y[start:start + effective_length].astype(np.float32)\n",
        "            else:\n",
        "                y = y.astype(np.float32)\n",
        "\n",
        "        labels = np.zeros(len(BIRD_CODE), dtype=\"f\")\n",
        "        labels[BIRD_CODE[ebird_code]] = 1\n",
        "\n",
        "        return {\"waveform\": y, \"targets\": labels}"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-04-07T12:26:45.097761Z",
          "iopub.execute_input": "2024-04-07T12:26:45.098087Z",
          "iopub.status.idle": "2024-04-07T12:26:45.111914Z",
          "shell.execute_reply.started": "2024-04-07T12:26:45.098059Z",
          "shell.execute_reply": "2024-04-07T12:26:45.110747Z"
        },
        "trusted": true,
        "id": "pBkYLvOFOzPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criterion"
      ],
      "metadata": {
        "id": "z5Uh4hMnOzPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PANNsLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bce = nn.BCELoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_ = input[\"clipwise_output\"]\n",
        "        input_ = torch.where(torch.isnan(input_),\n",
        "                             torch.zeros_like(input_),\n",
        "                             input_)\n",
        "        input_ = torch.where(torch.isinf(input_),\n",
        "                             torch.zeros_like(input_),\n",
        "                             input_)\n",
        "\n",
        "        target = target.float()\n",
        "\n",
        "        return self.bce(input_, target)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-04-07T12:26:48.960454Z",
          "iopub.execute_input": "2024-04-07T12:26:48.960801Z",
          "iopub.status.idle": "2024-04-07T12:26:48.968744Z",
          "shell.execute_reply.started": "2024-04-07T12:26:48.960772Z",
          "shell.execute_reply": "2024-04-07T12:26:48.967648Z"
        },
        "trusted": true,
        "id": "YPq-zvk8OzPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callbacks"
      ],
      "metadata": {
        "id": "HSl3e8VtOzPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class F1Callback(Callback):\n",
        "    def __init__(self,\n",
        "                 input_key: str = \"targets\",\n",
        "                 output_key: str = \"logits\",\n",
        "                 model_output_key: str = \"clipwise_output\",\n",
        "                 prefix: str = \"f1\"):\n",
        "        super().__init__(CallbackOrder.Metric)\n",
        "\n",
        "        self.input_key = input_key\n",
        "        self.output_key = output_key\n",
        "        self.model_output_key = model_output_key\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def on_loader_start(self, state: State):\n",
        "        self.prediction: List[np.ndarray] = []\n",
        "        self.target: List[np.ndarray] = []\n",
        "\n",
        "    def on_batch_end(self, state: State):\n",
        "        targ = state.input[self.input_key].detach().cpu().numpy()\n",
        "        out = state.output[self.output_key]\n",
        "\n",
        "        clipwise_output = out[self.model_output_key].detach().cpu().numpy()\n",
        "\n",
        "        self.prediction.append(clipwise_output)\n",
        "        self.target.append(targ)\n",
        "\n",
        "        y_pred = clipwise_output.argmax(axis=1)\n",
        "        y_true = targ.argmax(axis=1)\n",
        "\n",
        "        score = f1_score(y_true, y_pred, average=\"macro\")\n",
        "        state.batch_metrics[self.prefix] = score\n",
        "\n",
        "    def on_loader_end(self, state: State):\n",
        "        y_pred = np.concatenate(self.prediction, axis=0).argmax(axis=1)\n",
        "        y_true = np.concatenate(self.target, axis=0).argmax(axis=1)\n",
        "        score = f1_score(y_true, y_pred, average=\"macro\")\n",
        "        state.loader_metrics[self.prefix] = score\n",
        "        if state.is_valid_loader:\n",
        "            state.epoch_metrics[state.valid_loader + \"_epoch_\" +\n",
        "                                self.prefix] = score\n",
        "        else:\n",
        "            state.epoch_metrics[\"train_epoch_\" + self.prefix] = score\n",
        "\n",
        "\n",
        "class mAPCallback(Callback):\n",
        "    def __init__(self,\n",
        "                 input_key: str = \"targets\",\n",
        "                 output_key: str = \"logits\",\n",
        "                 model_output_key: str = \"clipwise_output\",\n",
        "                 prefix: str = \"mAP\"):\n",
        "        super().__init__(CallbackOrder.Metric)\n",
        "        self.input_key = input_key\n",
        "        self.output_key = output_key\n",
        "        self.model_output_key = model_output_key\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def on_loader_start(self, state: State):\n",
        "        self.prediction: List[np.ndarray] = []\n",
        "        self.target: List[np.ndarray] = []\n",
        "\n",
        "    def on_batch_end(self, state: State):\n",
        "        targ = state.input[self.input_key].detach().cpu().numpy()\n",
        "        out = state.output[self.output_key]\n",
        "\n",
        "        clipwise_output = out[self.model_output_key].detach().cpu().numpy()\n",
        "\n",
        "        self.prediction.append(clipwise_output)\n",
        "        self.target.append(targ)\n",
        "\n",
        "        score = average_precision_score(targ, clipwise_output, average=None)\n",
        "        score = np.nan_to_num(score).mean()\n",
        "        state.batch_metrics[self.prefix] = score\n",
        "\n",
        "    def on_loader_end(self, state: State):\n",
        "        y_pred = np.concatenate(self.prediction, axis=0)\n",
        "        y_true = np.concatenate(self.target, axis=0)\n",
        "        score = average_precision_score(y_true, y_pred, average=None)\n",
        "        score = np.nan_to_num(score).mean()\n",
        "        state.loader_metrics[self.prefix] = score\n",
        "        if state.is_valid_loader:\n",
        "            state.epoch_metrics[state.valid_loader + \"_epoch_\" +\n",
        "                                self.prefix] = score\n",
        "        else:\n",
        "            state.epoch_metrics[\"train_epoch_\" + self.prefix] = score"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-04-07T12:26:54.503889Z",
          "iopub.execute_input": "2024-04-07T12:26:54.504177Z",
          "iopub.status.idle": "2024-04-07T12:26:54.532362Z",
          "shell.execute_reply.started": "2024-04-07T12:26:54.50415Z",
          "shell.execute_reply": "2024-04-07T12:26:54.531572Z"
        },
        "trusted": true,
        "id": "Nm6ZONVtOzPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train\n",
        "\n",
        "Some code are taken from https://www.kaggle.com/ttahara/training-birdsong-baseline-resnest50-fast .\n",
        "Thanks @ttahara!"
      ],
      "metadata": {
        "id": "c3pb8SheOzPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_list = []\n",
        "for audio_d in TRAIN_RESAMPLED_AUDIO_DIRS:\n",
        "    if not audio_d.exists():\n",
        "        continue\n",
        "    for ebird_d in audio_d.iterdir():\n",
        "        if ebird_d.is_file():\n",
        "            continue\n",
        "        for wav_f in ebird_d.iterdir():\n",
        "            tmp_list.append([ebird_d.name, wav_f.name, wav_f.as_posix()])\n",
        "\n",
        "train_wav_path_exist = pd.DataFrame(\n",
        "    tmp_list, columns=[\"ebird_code\", \"resampled_filename\", \"file_path\"])\n",
        "\n",
        "del tmp_list\n",
        "\n",
        "train_all = pd.merge(\n",
        "    train, train_wav_path_exist, on=[\"ebird_code\", \"resampled_filename\"], how=\"inner\")\n",
        "\n",
        "print(train.shape)\n",
        "print(train_wav_path_exist.shape)\n",
        "print(train_all.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:26:59.142376Z",
          "iopub.execute_input": "2024-04-07T12:26:59.142705Z",
          "iopub.status.idle": "2024-04-07T12:26:59.693564Z",
          "shell.execute_reply.started": "2024-04-07T12:26:59.142676Z",
          "shell.execute_reply": "2024-04-07T12:26:59.692529Z"
        },
        "trusted": true,
        "id": "9GhQ2CdeOzPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "train_all[\"fold\"] = -1\n",
        "for fold_id, (train_index, val_index) in enumerate(skf.split(train_all, train_all[\"ebird_code\"])):\n",
        "    train_all.iloc[val_index, -1] = fold_id\n",
        "\n",
        "# # check the propotion\n",
        "fold_proportion = pd.pivot_table(train_all, index=\"ebird_code\", columns=\"fold\", values=\"xc_id\", aggfunc=len)\n",
        "print(fold_proportion.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:27:02.563501Z",
          "iopub.execute_input": "2024-04-07T12:27:02.563852Z",
          "iopub.status.idle": "2024-04-07T12:27:02.643194Z",
          "shell.execute_reply.started": "2024-04-07T12:27:02.56382Z",
          "shell.execute_reply": "2024-04-07T12:27:02.642282Z"
        },
        "trusted": true,
        "id": "IZ3vMwpHOzPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_fold = 0\n",
        "train_file_list = train_all.query(\"fold != @use_fold\")[[\"file_path\", \"ebird_code\"]].values.tolist()\n",
        "val_file_list = train_all.query(\"fold == @use_fold\")[[\"file_path\", \"ebird_code\"]].values.tolist()\n",
        "\n",
        "print(\"[fold {}] train: {}, val: {}\".format(use_fold, len(train_file_list), len(val_file_list)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:27:06.484377Z",
          "iopub.execute_input": "2024-04-07T12:27:06.484712Z",
          "iopub.status.idle": "2024-04-07T12:27:06.520144Z",
          "shell.execute_reply.started": "2024-04-07T12:27:06.484683Z",
          "shell.execute_reply": "2024-04-07T12:27:06.519305Z"
        },
        "trusted": true,
        "id": "NLgI8ZeeOzPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# loaders\n",
        "loaders = {\n",
        "    \"train\": data.DataLoader(PANNsDataset(train_file_list, None),\n",
        "                             batch_size=64,\n",
        "                             shuffle=True,\n",
        "                             num_workers=2,\n",
        "                             pin_memory=True,\n",
        "                             drop_last=True),\n",
        "    \"valid\": data.DataLoader(PANNsDataset(val_file_list, None),\n",
        "                             batch_size=64,\n",
        "                             shuffle=False,\n",
        "                             num_workers=2,\n",
        "                             pin_memory=True,\n",
        "                             drop_last=False)\n",
        "}\n",
        "\n",
        "# model\n",
        "model_config[\"classes_num\"] = 527\n",
        "model = PANNsCNN14Att(**model_config)\n",
        "weights = torch.load(\"../input/pannscnn14-decisionlevelatt-weight/Cnn14_DecisionLevelAtt_mAP0.425.pth\")\n",
        "# Fixed in V3\n",
        "model.load_state_dict(weights[\"model\"])\n",
        "model.att_block = AttBlock(2048, 264, activation='sigmoid')\n",
        "model.att_block.init_weights()\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Scheduler\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "# Loss\n",
        "criterion = PANNsLoss().to(device)\n",
        "\n",
        "# callbacks\n",
        "callbacks = [\n",
        "    F1Callback(input_key=\"targets\", output_key=\"logits\", prefix=\"f1\"),\n",
        "    mAPCallback(input_key=\"targets\", output_key=\"logits\", prefix=\"mAP\"),\n",
        "    CheckpointCallback(save_n_best=0)\n",
        "]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:27:09.586093Z",
          "iopub.execute_input": "2024-04-07T12:27:09.586556Z",
          "iopub.status.idle": "2024-04-07T12:27:11.424505Z",
          "shell.execute_reply.started": "2024-04-07T12:27:09.586513Z",
          "shell.execute_reply": "2024-04-07T12:27:11.423644Z"
        },
        "trusted": true,
        "id": "MP5sL3eBOzPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "runner = SupervisedRunner(\n",
        "    device=device,\n",
        "    input_key=\"waveform\",\n",
        "    input_target_key=\"targets\")\n",
        "\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    loaders=loaders,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=10,\n",
        "    verbose=True,\n",
        "    logdir=f\"fold0\",\n",
        "    callbacks=callbacks,\n",
        "    main_metric=\"epoch_f1\",\n",
        "    minimize_metric=False)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-04-07T12:27:16.086489Z",
          "iopub.execute_input": "2024-04-07T12:27:16.086817Z",
          "iopub.status.idle": "2024-04-07T12:43:16.944936Z",
          "shell.execute_reply.started": "2024-04-07T12:27:16.086787Z",
          "shell.execute_reply": "2024-04-07T12:43:16.940186Z"
        },
        "trusted": true,
        "id": "D-f3hfzxOzPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems it's learning something.\n",
        "\n",
        "Now I'll show how this model works in the inference phase. I'll use trained model of this which I trained by myself using the data of this competition in my local environment.\n",
        "\n",
        "Since [several concerns](https://www.kaggle.com/c/birdsong-recognition/discussion/172356) are expressed about over-sharing of top solutions during competition, and since I do respect those people who have worked hard to improve their scores, I would not make trained weight in common and would not share how I trained this model."
      ],
      "metadata": {
        "id": "TuIYjfb8OzPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction with SED model"
      ],
      "metadata": {
        "id": "pZDkmLk7OzPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    \"sample_rate\": 32000,\n",
        "    \"window_size\": 1024,\n",
        "    \"hop_size\": 320,\n",
        "    \"mel_bins\": 64,\n",
        "    \"fmin\": 50,\n",
        "    \"fmax\": 14000,\n",
        "    \"classes_num\": 264\n",
        "}\n",
        "\n",
        "weights_path = \"../input/birdcall-pannsatt-aux-weak/best.pth\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:43:16.94651Z",
          "iopub.status.idle": "2024-04-07T12:43:16.946978Z"
        },
        "trusted": true,
        "id": "TZwc2cFYOzPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(config: dict, weights_path: str):\n",
        "    model = PANNsCNN14Att(**config)\n",
        "    checkpoint = torch.load(weights_path)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:43:16.953901Z",
          "iopub.status.idle": "2024-04-07T12:43:16.954357Z"
        },
        "trusted": true,
        "id": "QOlPIcZZOzPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_for_clip(test_df: pd.DataFrame,\n",
        "                        clip: np.ndarray,\n",
        "                        model: PANNsCNN14Att,\n",
        "                        threshold=0.5):\n",
        "    PERIOD = 30\n",
        "    audios = []\n",
        "    y = clip.astype(np.float32)\n",
        "    len_y = len(y)\n",
        "    start = 0\n",
        "    end = PERIOD * SR\n",
        "    while True:\n",
        "        y_batch = y[start:end].astype(np.float32)\n",
        "        if len(y_batch) != PERIOD * SR:\n",
        "            y_pad = np.zeros(PERIOD * SR, dtype=np.float32)\n",
        "            y_pad[:len(y_batch)] = y_batch\n",
        "            audios.append(y_pad)\n",
        "            break\n",
        "        start = end\n",
        "        end += PERIOD * SR\n",
        "        audios.append(y_batch)\n",
        "\n",
        "    array = np.asarray(audios)\n",
        "    tensors = torch.from_numpy(array)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.eval()\n",
        "    estimated_event_list = []\n",
        "    global_time = 0.0\n",
        "    site = test_df[\"site\"].values[0]\n",
        "    audio_id = test_df[\"audio_id\"].values[0]\n",
        "    for image in progress_bar(tensors):\n",
        "        image = image.view(1, image.size(0))\n",
        "        image = image.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = model(image)\n",
        "            framewise_outputs = prediction[\"framewise_output\"].detach(\n",
        "                ).cpu().numpy()[0]\n",
        "\n",
        "        thresholded = framewise_outputs >= threshold\n",
        "\n",
        "        for target_idx in range(thresholded.shape[1]):\n",
        "            if thresholded[:, target_idx].mean() == 0:\n",
        "                pass\n",
        "            else:\n",
        "                detected = np.argwhere(thresholded[:, target_idx]).reshape(-1)\n",
        "                head_idx = 0\n",
        "                tail_idx = 0\n",
        "                while True:\n",
        "                    if (tail_idx + 1 == len(detected)) or (\n",
        "                            detected[tail_idx + 1] -\n",
        "                            detected[tail_idx] != 1):\n",
        "                        onset = 0.01 * detected[\n",
        "                            head_idx] + global_time\n",
        "                        offset = 0.01 * detected[\n",
        "                            tail_idx] + global_time\n",
        "                        onset_idx = detected[head_idx]\n",
        "                        offset_idx = detected[tail_idx]\n",
        "                        max_confidence = framewise_outputs[\n",
        "                            onset_idx:offset_idx, target_idx].max()\n",
        "                        mean_confidence = framewise_outputs[\n",
        "                            onset_idx:offset_idx, target_idx].mean()\n",
        "                        estimated_event = {\n",
        "                            \"site\": site,\n",
        "                            \"audio_id\": audio_id,\n",
        "                            \"ebird_code\": INV_BIRD_CODE[target_idx],\n",
        "                            \"onset\": onset,\n",
        "                            \"offset\": offset,\n",
        "                            \"max_confidence\": max_confidence,\n",
        "                            \"mean_confidence\": mean_confidence\n",
        "                        }\n",
        "                        estimated_event_list.append(estimated_event)\n",
        "                        head_idx = tail_idx + 1\n",
        "                        tail_idx = tail_idx + 1\n",
        "                        if head_idx >= len(detected):\n",
        "                            break\n",
        "                    else:\n",
        "                        tail_idx += 1\n",
        "        global_time += PERIOD\n",
        "\n",
        "    prediction_df = pd.DataFrame(estimated_event_list)\n",
        "    return prediction_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:43:16.958583Z",
          "iopub.status.idle": "2024-04-07T12:43:16.959016Z"
        },
        "trusted": true,
        "id": "CKNRSj-dOzPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(test_df: pd.DataFrame,\n",
        "               test_audio: Path,\n",
        "               model_config: dict,\n",
        "               weights_path: str,\n",
        "               threshold=0.5):\n",
        "    model = get_model(model_config, weights_path)\n",
        "    unique_audio_id = test_df.audio_id.unique()\n",
        "\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    prediction_dfs = []\n",
        "    for audio_id in unique_audio_id:\n",
        "        with timer(f\"Loading {audio_id}\"):\n",
        "            clip, _ = librosa.load(test_audio / (audio_id + \".mp3\"),\n",
        "                                   sr=SR,\n",
        "                                   mono=True,\n",
        "                                   res_type=\"kaiser_fast\")\n",
        "\n",
        "        test_df_for_audio_id = test_df.query(\n",
        "            f\"audio_id == '{audio_id}'\").reset_index(drop=True)\n",
        "        with timer(f\"Prediction on {audio_id}\"):\n",
        "            prediction_df = prediction_for_clip(test_df_for_audio_id,\n",
        "                                                clip=clip,\n",
        "                                                model=model,\n",
        "                                                threshold=threshold)\n",
        "\n",
        "        prediction_dfs.append(prediction_df)\n",
        "\n",
        "    prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n",
        "    return prediction_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:43:16.961463Z",
          "iopub.status.idle": "2024-04-07T12:43:16.961898Z"
        },
        "trusted": true,
        "id": "rdp55pRROzPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_df = prediction(test_df=test,\n",
        "                           test_audio=TEST_AUDIO_DIR,\n",
        "                           model_config=model_config,\n",
        "                           weights_path=weights_path,\n",
        "                           threshold=0.5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:43:16.962857Z",
          "iopub.status.idle": "2024-04-07T12:43:16.963323Z"
        },
        "trusted": true,
        "id": "u4iHQFlrOzPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:43:16.964358Z",
          "iopub.status.idle": "2024-04-07T12:43:16.964786Z"
        },
        "trusted": true,
        "id": "BJxnK3oOOzPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postprocess\n"
      ],
      "metadata": {
        "id": "7g8U5ePcOzPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {}\n",
        "\n",
        "for audio_id, sub_df in prediction_df.groupby(\"audio_id\"):\n",
        "    events = sub_df[[\"ebird_code\", \"onset\", \"offset\", \"max_confidence\", \"site\"]].values\n",
        "    n_events = len(events)\n",
        "    removed_event = []\n",
        "    # Overlap deletion: this part may not be necessary\n",
        "    # I deleted this part in other model and found there's no difference on the public LB score.\n",
        "    for i in range(n_events):\n",
        "        for j in range(n_events):\n",
        "            if i == j:\n",
        "                continue\n",
        "            if i in removed_event:\n",
        "                continue\n",
        "            if j in removed_event:\n",
        "                continue\n",
        "\n",
        "            event_i = events[i]\n",
        "            event_j = events[j]\n",
        "\n",
        "            if (event_i[1] - event_j[2] >= 0) or (event_j[1] - event_i[2] >= 0):\n",
        "                pass\n",
        "            else:\n",
        "                later_onset = max(event_i[1], event_j[1])\n",
        "                sooner_onset = min(event_i[1], event_j[1])\n",
        "                sooner_offset = min(event_i[2], event_j[2])\n",
        "                later_offset = max(event_i[2], event_j[2])\n",
        "\n",
        "                intersection = sooner_offset - later_onset\n",
        "                union = later_offset - sooner_onset\n",
        "\n",
        "                iou = intersection / union\n",
        "                if iou > 0.4:\n",
        "                    if event_i[3] > event_j[3]:\n",
        "                        removed_event.append(j)\n",
        "                    else:\n",
        "                        removed_event.append(i)\n",
        "\n",
        "    site = events[0][4]\n",
        "    for i in range(n_events):\n",
        "        if i in removed_event:\n",
        "            continue\n",
        "        event = events[i][0]\n",
        "        onset = events[i][1]\n",
        "        offset = events[i][2]\n",
        "        if site in {\"site_1\", \"site_2\"}:\n",
        "            start_section = int((onset // 5) * 5) + 5\n",
        "            end_section = int((offset // 5) * 5) + 5\n",
        "            cur_section = start_section\n",
        "\n",
        "            row_id = f\"{site}_{audio_id}_{start_section}\"\n",
        "            if labels.get(row_id) is not None:\n",
        "                labels[row_id].add(event)\n",
        "            else:\n",
        "                labels[row_id] = set()\n",
        "                labels[row_id].add(event)\n",
        "\n",
        "            while cur_section != end_section:\n",
        "                cur_section += 5\n",
        "                row_id = f\"{site}_{audio_id}_{cur_section}\"\n",
        "                if labels.get(row_id) is not None:\n",
        "                    labels[row_id].add(event)\n",
        "                else:\n",
        "                    labels[row_id] = set()\n",
        "                    labels[row_id].add(event)\n",
        "        else:\n",
        "            row_id = f\"{site}_{audio_id}\"\n",
        "            if labels.get(row_id) is not None:\n",
        "                labels[row_id].add(event)\n",
        "            else:\n",
        "                labels[row_id] = set()\n",
        "                labels[row_id].add(event)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:43:16.965878Z",
          "iopub.status.idle": "2024-04-07T12:43:16.966342Z"
        },
        "trusted": true,
        "id": "rvMDOsBXOzPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in labels:\n",
        "    labels[key] = \" \".join(sorted(list(labels[key])))\n",
        "\n",
        "\n",
        "row_ids = list(labels.keys())\n",
        "birds = list(labels.values())\n",
        "post_processed = pd.DataFrame({\n",
        "    \"row_id\": row_ids,\n",
        "    \"birds\": birds\n",
        "})\n",
        "post_processed.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:43:16.969084Z",
          "iopub.status.idle": "2024-04-07T12:43:16.969556Z"
        },
        "trusted": true,
        "id": "e_ZQodH7OzPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_row_id = test[[\"row_id\"]]\n",
        "submission = all_row_id.merge(post_processed, on=\"row_id\", how=\"left\")\n",
        "submission = submission.fillna(\"nocall\")\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "submission.head(20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-07T12:43:16.975015Z",
          "iopub.status.idle": "2024-04-07T12:43:16.975474Z"
        },
        "trusted": true,
        "id": "htavSZMsOzPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EOF"
      ],
      "metadata": {
        "id": "51cx5LwrOzPs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "YTiDkQAwOzPs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}